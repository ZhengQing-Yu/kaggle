{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lrGjRg_CLgs"
      },
      "source": [
        "# How to run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmTwoa3CLgz"
      },
      "source": [
        "In directory containing this notebook, expected to find\n",
        "- train.csv \n",
        "- folder noisy-images containing the images\n",
        "\n",
        "Run cells sequentially"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GSZaojHyCMeg"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# !unzip images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRbZQqvxCLg4",
        "outputId": "7b892f58-f0ee-4dfa-9320-81a340ca4bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Model, losses, layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-jIi14CLg_"
      },
      "source": [
        "### Get csv data and convert text embedding into np array "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6sn5h5X2CLhC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train_preproc.csv\", delimiter=',')\n",
        "\n",
        "df['noisyTextDescription'] = df['noisyTextDescription'].map(lambda x: np.array(x[1:-1].split(), dtype=float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6ZSDbJgCLhF"
      },
      "source": [
        "### Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wv7voK4sCLhI"
      },
      "outputs": [],
      "source": [
        "def load_image(id):\n",
        "    img = Image.open('noisy-images/{id}.jpg'.format(id=id))\n",
        "    data = np.asarray(img)\n",
        "    return data/255\n",
        "\n",
        "df[\"noisyImage\"] = df.apply(lambda x: load_image(x['id']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj8W2XUhCLhK"
      },
      "source": [
        "### Get unique elements in categorical inputs \n",
        "- category\n",
        "- gender\n",
        "- baseColour\n",
        "- season\n",
        "- usage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZgELOMvCLhN"
      },
      "source": [
        "Categorical features are embedded as a one-hot tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcTj0PXGCLhT",
        "outputId": "45594834-0ef8-45cc-f979-3f982cf90089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27 possible values for category\n",
            "['Sandal', 'Bottomwear', 'Shoes', 'Topwear', 'Innerwear', 'Loungewear and Nightwear', 'Watches', 'Fragrance', 'Eyewear', 'Lips', 'Bags', 'Saree', 'Wallets', 'Scarves', 'Jewellery', 'Dress', 'Ties', 'Flip Flops', 'Headwear', 'Makeup', 'Belts', 'Socks', 'Nails', 'Free Gifts', 'Apparel Set', 'Cufflinks', 'Accessories']\n",
            "5 possible values for gender\n",
            "['Men', 'Women', 'Girls', 'Unisex', 'Boys']\n",
            "46 possible values for baseColour\n",
            "['Tan', 'Blue', 'White', 'Black', 'Beige', 'Pink', 'Green', 'Red', 'Brown', 'Grey', 'Yellow', 'Magenta', 'Steel', 'Purple', 'Orange', 'Silver', 'Navy Blue', 'Maroon', 'Gold', 'Olive', 'Cream', 'Peach', 'Lavender', 'Coffee Brown', 'Grey Melange', 'Teal', 'Rust', 'Multi', 'Charcoal', 'Turquoise Blue', 'Rose', 'Off White', 'Skin', 'Khaki', 'Metallic', 'Nude', 'Mustard', 'Copper', 'Burgundy', 'Sea Green', 'Mauve', 'Mushroom Brown', 'Bronze', 'Taupe', 'Lime Green', 'Fluorescent Green']\n",
            "4 possible values for season\n",
            "['Summer', 'Fall', 'Winter', 'Spring']\n",
            "7 possible values for usage\n",
            "['Casual', 'Sports', 'Formal', 'Ethnic', 'Smart Casual', 'Party', 'Travel']\n"
          ]
        }
      ],
      "source": [
        "def get_categorical_embedding(categorical_input, silent=False):\n",
        "    possible_values = df[categorical_input].unique().tolist()\n",
        "    num_possible_values = len(possible_values)\n",
        "\n",
        "    index = lambda x: possible_values.index(x)\n",
        "    def encoder(x: int):\n",
        "        encoding = np.zeros(num_possible_values)\n",
        "        encoding[x] = 1\n",
        "        return np.ndarray.copy(encoding)\n",
        "\n",
        "    # index = layers.StringLookup(vocabulary = possible_values, output_mode = 'int')\n",
        "    # encoder = layers.CategoryEncoding(num_tokens=num_possible_values + 1, output_mode='one_hot')\n",
        "\n",
        "    \n",
        "    if not silent:\n",
        "        print(\"{n} possible values for {name}\".format(n = num_possible_values, name = categorical_input))\n",
        "        print(possible_values)\n",
        "    \n",
        "    return lambda feature: encoder(index(feature))\n",
        "\n",
        "\n",
        "# for categorical_input in ['category', 'gender', 'baseColour', 'season', 'usage']:\n",
        "#     possible_values = data_points[categorical_input].unique()\n",
        "#     num_possible_values = len(possible_values)\n",
        "\n",
        "#     print(\"{n} possible values for {name}\".format(n = num_possible_values, name = categorical_input))\n",
        "#     print(possible_values)\n",
        "    \n",
        "category_embedding = get_categorical_embedding('category')\n",
        "gender_embedding = get_categorical_embedding('gender')\n",
        "baseColour_embedding = get_categorical_embedding('baseColour')\n",
        "season_embedding = get_categorical_embedding('season')\n",
        "usage_embedding = get_categorical_embedding('usage')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lYSbbUWCLhZ"
      },
      "source": [
        "### Encode categorical "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtDeDtBgCLhd",
        "outputId": "291e30e9-0083-4530-b506-6e9b43ee2116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27 possible categories\n",
            "Sandal : [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Bottomwear : [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Shoes : [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Topwear : [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Innerwear : [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Loungewear and Nightwear : [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Watches : [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Fragrance : [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Eyewear : [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Lips : [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Bags : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Saree : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Wallets : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Scarves : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Jewellery : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Dress : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Ties : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Flip Flops : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Headwear : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Makeup : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Belts : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Socks : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0.]\n",
            "Nails : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0.]\n",
            "Free Gifts : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0.]\n",
            "Apparel Set : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0.]\n",
            "Cufflinks : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0.]\n",
            "Accessories : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1.]\n"
          ]
        }
      ],
      "source": [
        "possible_categories = df['category'].unique()\n",
        "\n",
        "print('{n} possible categories'.format(n=len(possible_categories)))\n",
        "\n",
        "\n",
        "for category in possible_categories:\n",
        "    print('{category} : {embedding}'.format(category=category, embedding=category_embedding(category)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ytZDVsYACLhh"
      },
      "outputs": [],
      "source": [
        "df['category'] = df['category'].map(lambda x: category_embedding(x))\n",
        "df['gender'] = df['gender'].map(lambda x: gender_embedding(x))\n",
        "df['baseColour'] = df['baseColour'].map(lambda x: baseColour_embedding(x))\n",
        "df['season'] = df['season'].map(lambda x: season_embedding(x))\n",
        "df['usage'] = df['usage'].map(lambda x: usage_embedding(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uyoU4JfNCLhj"
      },
      "outputs": [],
      "source": [
        "df[\"categoricalData\"] = df.apply(lambda x: np.concatenate((x['gender'], x['baseColour'], x['season'], x['usage']), axis=0), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrDf6fURCLhl"
      },
      "source": [
        "### Separate into validationing and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy_eMTMECLhn",
        "outputId": "49c0569b-96c2-468d-d08a-9ff29fae109c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17301 items in training set\n",
            "4326 items in validation set\n"
          ]
        }
      ],
      "source": [
        "# separate into training and validation 80/20 ratio\n",
        "# sample(frac=1) randomly shuffles data\n",
        "train_dataframe, validation_dataframe = np.split(df.sample(frac=1, random_state=480), [int(0.8 * len(df))])\n",
        "\n",
        "\n",
        "print(\"{n} items in training set\".format(n = len(train_dataframe)))\n",
        "print(\"{n} items in validation set\".format(n = len(validation_dataframe)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wu3qRe5hCLht"
      },
      "outputs": [],
      "source": [
        "y_train = train_dataframe['category'].to_numpy()\n",
        "\n",
        "x_text_train = train_dataframe['noisyTextDescription'].to_numpy()\n",
        "x_img_train = train_dataframe['noisyImage'].to_numpy()\n",
        "x_categorical_train = train_dataframe['categoricalData'].to_numpy()\n",
        "\n",
        "\n",
        "y_validation = validation_dataframe['category'].to_numpy()\n",
        "\n",
        "x_text_validation = validation_dataframe['noisyTextDescription'].to_numpy()\n",
        "x_img_validation = validation_dataframe['noisyImage'].to_numpy()\n",
        "x_categorical_validation = validation_dataframe['categoricalData'].to_numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2MH1gUCLhv",
        "outputId": "43ea34c3-a479-4d5c-a348-0d94567094ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(17301, 80, 60, 3)\n",
            "(17301, 768)\n",
            "(17301, 62)\n",
            "(4326, 80, 60, 3)\n",
            "(4326, 768)\n",
            "(4326, 62)\n",
            "(17301, 27)\n",
            "(4326, 27)\n"
          ]
        }
      ],
      "source": [
        "x_img_train = np.vstack([np.array([img]) for img in x_img_train])\n",
        "print(x_img_train.shape)\n",
        "\n",
        "x_text_train = np.vstack([np.array([text]) for text in x_text_train])\n",
        "print(x_text_train.shape)\n",
        "\n",
        "x_categorical_train = np.vstack([np.array([categorical]) for categorical in x_categorical_train])\n",
        "print(x_categorical_train.shape)\n",
        "\n",
        "\n",
        "x_img_validation = np.vstack([np.array([img]) for img in x_img_validation])\n",
        "x_text_validation = np.vstack([np.array([text]) for text in x_text_validation])\n",
        "x_categorical_validation = np.vstack([np.array([categorical]) for categorical in x_categorical_validation])\n",
        "\n",
        "\n",
        "\n",
        "print(x_img_validation.shape)\n",
        "print(x_text_validation.shape)\n",
        "print(x_categorical_validation.shape)\n",
        "\n",
        "\n",
        "y_train = np.vstack([np.array([y]) for y in y_train])\n",
        "print(y_train.shape)\n",
        "\n",
        "y_validation = np.vstack([np.array([y]) for y in y_validation])\n",
        "print(y_validation.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5Uau28zsNFXQ"
      },
      "outputs": [],
      "source": [
        "del df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU2rvLFxCLhx"
      },
      "source": [
        "### Define NN models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pbIFXf1HCLh0"
      },
      "outputs": [],
      "source": [
        "# Model: CNN for image (4 conv layers with skip connection -> maxpool 2 by 2)\n",
        "# Model: dense linear for categorical attributes\n",
        "# Model: transformer for text \n",
        "\n",
        "# flatten CNN, Linear, Transformer into a 1D layer\n",
        "# linear layers -> softmax\n",
        "\n",
        "class ResidualCNNLayer(layers.Layer):\n",
        "    def __init__(self, n_filters=64, kernel_size=3) -> None:\n",
        "        super(ResidualCNNLayer, self).__init__()\n",
        "        self.conv1 = layers.Conv2D(filters=n_filters/2, kernel_size=kernel_size, padding='same')\n",
        "        self.conv2 = layers.Conv2D(filters=n_filters/2, kernel_size=kernel_size, padding='same')\n",
        "        self.conv3 = layers.Conv2D(filters=n_filters, kernel_size=kernel_size, padding='same')\n",
        "        self.batch_normalization = layers.BatchNormalization()\n",
        "        self.identity_mapping = layers.Conv2D(filters=n_filters/2, kernel_size=1, padding='same')\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x_skip = x\n",
        "        # Layer 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_normalization(x, training=training)\n",
        "        x = self.relu(x)\n",
        "        # Layer 2\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # Add Residue\n",
        "        x = layers.Add()([x, self.identity_mapping(x_skip)])\n",
        "\n",
        "        x = self.batch_normalization(x, training=training)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Img(layers.Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super(Img, self).__init__()\n",
        "        self.res1 = ResidualCNNLayer(n_filters=64, kernel_size=3)\n",
        "        self.res2 = ResidualCNNLayer(n_filters=128, kernel_size=3)\n",
        "        self.res3 = ResidualCNNLayer(n_filters=256, kernel_size=3)\n",
        "        self.res4 = ResidualCNNLayer(n_filters=512, kernel_size=3)\n",
        "        self.flattening_function = layers.GlobalAveragePooling2D()\n",
        "\n",
        "        # padding = same is important for last residual layer, as size is not divisible by 2\n",
        "        self.pool = layers.MaxPool2D(padding='same')\n",
        "        self.dropout = layers.Dropout(0.35)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.res1(x, training=training)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.res2(x, training=training)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.res3(x, training=training)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.res4(x, training=training)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        return self.flattening_function(x)\n",
        "\n",
        "\n",
        "class Txt(layers.Layer):\n",
        "    def __init__(self) -> None:\n",
        "        super(Txt, self).__init__()\n",
        "        self.dense1 = layers.Dense(256)\n",
        "\n",
        "        self.relu = layers.ReLU()\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.dense1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class NN(Model):\n",
        "    def __init__(self) -> None:\n",
        "        super(NN, self).__init__()\n",
        "        # for image\n",
        "        self.img_layer = Img()\n",
        "\n",
        "        # some processing on text embedding\n",
        "        # since the proprocessed transformer embedding is from a general model, want to reduce the size of embedding\n",
        "        self.txt_layer = Txt()\n",
        "\n",
        "        # dense layers after combining       \n",
        "        self.dense1 = layers.Dense(512, activation='relu')\n",
        "        # self.dense2 = layers.Dense(512, activation='relu')\n",
        "        # self.dense3 = layers.Dense(512, activation='relu')\n",
        "\n",
        "        self.dropout = layers.Dropout(0.5)\n",
        "\n",
        "        self.classification_layer = layers.Dense(27, activation='softmax')\n",
        "\n",
        "        self.concat = layers.Concatenate()\n",
        "\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        categorical, text, image = x\n",
        "        text = self.txt_layer(text, training=training)\n",
        "        image = self.img_layer(image, training=training)\n",
        "        combined = self.concat([categorical, text, image])\n",
        "        combined = self.dense1(combined)\n",
        "        combined = self.dropout(combined, training=training)\n",
        "        # combined = self.dense2(combined)\n",
        "        # combined = self.dropout(combined, training=training)\n",
        "        # combined = self.dense3(combined)\n",
        "        # combined = self.dropout(combined, training=training)\n",
        "        return self.classification_layer(combined)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BC7JcvSjCLh1"
      },
      "outputs": [],
      "source": [
        "model = NN()\n",
        "model.compile(optimizer='Adam', loss=losses.CategoricalCrossentropy(), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "K1mss6fkB_cw"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints/cp.ckpt\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, start_from_epoch=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yJPmteN56E9V"
      },
      "outputs": [],
      "source": [
        "tf.debugging.disable_traceback_filtering()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYWhEYfw6GXD",
        "outputId": "8fa4780c-63e0-4d01-ec8b-1af304547935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9b08af2520>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMtcoCHTCLh4",
        "outputId": "29fdac3d-36a7-4973-c9bf-b9a124f87b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 1.38941, saving model to checkpoints/cp.ckpt\n",
            "271/271 - 52s - loss: 1.4358 - accuracy: 0.6234 - val_loss: 1.3894 - val_accuracy: 0.6579 - 52s/epoch - 190ms/step\n",
            "Epoch 2/20\n",
            "\n",
            "Epoch 2: val_loss improved from 1.38941 to 1.18043, saving model to checkpoints/cp.ckpt\n",
            "271/271 - 27s - loss: 0.8106 - accuracy: 0.7840 - val_loss: 1.1804 - val_accuracy: 0.6993 - 27s/epoch - 101ms/step\n",
            "Epoch 3/20\n",
            "\n",
            "Epoch 3: val_loss improved from 1.18043 to 1.06523, saving model to checkpoints/cp.ckpt\n",
            "271/271 - 27s - loss: 0.6023 - accuracy: 0.8316 - val_loss: 1.0652 - val_accuracy: 0.7168 - 27s/epoch - 99ms/step\n",
            "Epoch 4/20\n",
            "\n",
            "Epoch 4: val_loss did not improve from 1.06523\n",
            "271/271 - 27s - loss: 0.5093 - accuracy: 0.8587 - val_loss: 1.1317 - val_accuracy: 0.6882 - 27s/epoch - 98ms/step\n",
            "Epoch 5/20\n",
            "\n",
            "Epoch 5: val_loss improved from 1.06523 to 1.05894, saving model to checkpoints/cp.ckpt\n",
            "271/271 - 28s - loss: 0.4467 - accuracy: 0.8725 - val_loss: 1.0589 - val_accuracy: 0.7050 - 28s/epoch - 103ms/step\n",
            "Epoch 6/20\n",
            "\n",
            "Epoch 6: val_loss improved from 1.05894 to 1.01398, saving model to checkpoints/cp.ckpt\n",
            "271/271 - 27s - loss: 0.3850 - accuracy: 0.8880 - val_loss: 1.0140 - val_accuracy: 0.7215 - 27s/epoch - 99ms/step\n",
            "Epoch 7/20\n",
            "\n",
            "Epoch 7: val_loss did not improve from 1.01398\n",
            "271/271 - 27s - loss: 0.3484 - accuracy: 0.8986 - val_loss: 1.1029 - val_accuracy: 0.7099 - 27s/epoch - 99ms/step\n",
            "Epoch 8/20\n",
            "\n",
            "Epoch 8: val_loss did not improve from 1.01398\n",
            "271/271 - 28s - loss: 0.3021 - accuracy: 0.9117 - val_loss: 1.1147 - val_accuracy: 0.7009 - 28s/epoch - 102ms/step\n",
            "Epoch 9/20\n",
            "\n",
            "Epoch 9: val_loss did not improve from 1.01398\n",
            "271/271 - 27s - loss: 0.2692 - accuracy: 0.9195 - val_loss: 1.1782 - val_accuracy: 0.6972 - 27s/epoch - 98ms/step\n",
            "Epoch 10/20\n",
            "\n",
            "Epoch 10: val_loss did not improve from 1.01398\n",
            "271/271 - 28s - loss: 0.2380 - accuracy: 0.9286 - val_loss: 1.1701 - val_accuracy: 0.7000 - 28s/epoch - 102ms/step\n"
          ]
        }
      ],
      "source": [
        "r = model.fit((x_categorical_train, x_text_train, x_img_train), y_train,\n",
        "              validation_data=((x_categorical_validation, x_text_validation, x_img_validation), y_validation),\n",
        "              verbose=2,\n",
        "              epochs=20,\n",
        "              batch_size=64,\n",
        "              callbacks=[stop_early, cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1kN29s4G84D",
        "outputId": "34e4c665-74bb-42ed-ac6b-9df3592d4c5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "updating: checkpoints/ (stored 0%)\n",
            "updating: checkpoints/checkpoint (deflated 38%)\n",
            "updating: checkpoints/.ipynb_checkpoints/ (stored 0%)\n",
            "updating: checkpoints/cp.ckpt.index (deflated 72%)\n",
            "updating: checkpoints/cp.ckpt.data-00000-of-00001 (deflated 28%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r checkpoints.zip ./checkpoints/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "pteuN9VsIh9r",
        "outputId": "2db7a0fb-63cd-454e-9766-ca2fea2c2cb7"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e66784fd-aaff-4501-b25a-1e86f3448321\", \"checkpoints.zip\", 102254812)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('checkpoints.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "L7wOmM4BCLh7",
        "outputId": "728be212-8424-4921-b54d-86d7b08cd155"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, residual_cnn_layer_4_layer_call_fn, residual_cnn_layer_4_layer_call_and_return_conditional_losses, residual_cnn_layer_5_layer_call_fn, residual_cnn_layer_5_layer_call_and_return_conditional_losses while saving (showing 5 of 71). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: nn_model5/ (stored 0%)\n",
            "  adding: nn_model5/variables/ (stored 0%)\n",
            "  adding: nn_model5/variables/variables.index (deflated 73%)\n",
            "  adding: nn_model5/variables/variables.data-00000-of-00001 (deflated 13%)\n",
            "  adding: nn_model5/assets/ (stored 0%)\n",
            "  adding: nn_model5/saved_model.pb (deflated 90%)\n",
            "  adding: nn_model5/keras_metadata.pb (deflated 92%)\n",
            "  adding: nn_model5/fingerprint.pb (stored 0%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_97bfc1b0-367e-4280-a4df-7d3b3166ebe6\", \"nn_model5.zip\", 40277788)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.load_weights(checkpoint_path)\n",
        "model.save('nn_model5')\n",
        "!zip -r nn_model5.zip ./nn_model5/\n",
        "files.download('nn_model5.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6WWwrYkCLh9",
        "outputId": "5883dfce-2bb0-487e-b202-c2d0fea9a535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"nn_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " img_1 (Img)                 multiple                  788384    \n",
            "                                                                 \n",
            " txt_1 (Txt)                 multiple                  196864    \n",
            "                                                                 \n",
            " dense_5 (Dense)             multiple                  10649088  \n",
            "                                                                 \n",
            " dense_6 (Dense)             multiple                  262656    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         multiple                  0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             multiple                  13851     \n",
            "                                                                 \n",
            " concatenate_1 (Concatenate)  multiple                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,910,843\n",
            "Trainable params: 11,910,395\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
